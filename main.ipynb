{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 23: Fast MRI Notebook\n",
    "Members: Sam Barrett, Bianca Bunaciu, Katie Potts, Aled, James Chamberlain, Vilmos\n",
    "Czeroczky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from layers.conv_layer import ConvLayer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model(s)\n",
    "1. Super resoution Convolutional Neural Network following the paper \"Learning a deep convolutional Network for image super-resolution\"\n",
    "2. UNET, based on the Facebook AI FastMRI paper (reference) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer of the network is formally expressed as:\n",
    "\n",
    "$$F_1(\\textbf{Y}) = \\max(0,W_1 * \\textbf{Y} + B_1)$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\textbf{Y}$ is our interpolated (undersampled) image\n",
    "- $\\textbf{X}$ is our ground truth image\n",
    "- $W_1$ is the filter\n",
    "    - of size $c\\times f_1 \\times f_1 \\times n_1$\n",
    "    - $c$ is the number of channels in the input image\n",
    "    - $f_1$ is the spacial size of the filter \n",
    "    - $n_1$ is the number of filters\n",
    "- $B_1$ is the bias\n",
    "\n",
    "- We want to recover from $\\textbf{Y}$ an image $F(\\textbf{Y}) \\approx \\textbf{X}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        c_0 = 1  # input channels\n",
    "        f_1 = 6  # spacial size of kernel\n",
    "        c_1 = 1  # output channels\n",
    "        n_1 = 64  # number of convs to apply\n",
    "        l1_modules: List[nn.Module] = []\n",
    "        l1_conv_layers = [nn.Conv2d(in_channels=c_0, out_channels=c_1, kernel_size=f_1, padding=1, stride=1) for _ in\n",
    "                          range(n_1)]\n",
    "        l1_modules.extend(l1_conv_layers)\n",
    "        l1_modules.append(nn.ReLU())\n",
    "\n",
    "        self.hidden1 = nn.Sequential(*l1_modules)\n",
    "\n",
    "        n_2 = 1  # number of convs to apply\n",
    "        c_2 = 32  # output channels\n",
    "        f_2 = 1  # size of kernel\n",
    "        l2_modules = []\n",
    "        l2_conv_layers = [nn.Conv2d(in_channels=c_1, out_channels=c_2, kernel_size=f_2, padding=1, stride=1) for _ in\n",
    "                          range(n_2)]\n",
    "        l2_modules.extend(l2_conv_layers)\n",
    "        l2_modules.append(nn.ReLU())\n",
    "\n",
    "        self.hidden2 = nn.Sequential(*l2_modules)\n",
    "\n",
    "        l3_modules = []\n",
    "        c_3 = c_0  # output channels = original input\n",
    "        n_3 = 1  # number of convolutions to apply\n",
    "        f_3 = 5  # size of kernel\n",
    "        l3_conv_layers = [nn.Conv2d(in_channels=c_2, out_channels=c_3, kernel_size=f_3, padding=1, stride=1) for _ in\n",
    "                          range(n_3)]\n",
    "        l3_modules.extend(l3_conv_layers)\n",
    "\n",
    "        self.hidden3 = nn.Sequential(*l3_modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hidden1(x)\n",
    "        print(f\"l1: {out.shape}\")\n",
    "        out = self.hidden2(out)\n",
    "        print(f\"l2: {out.shape}\")\n",
    "        out = self.hidden3(out)\n",
    "        print(f\"l3: {out.shape}\")\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in: int, c_out: int, c: int, n_pool_layers: int, drop_prob: float) -> None:\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.c = c\n",
    "        self.n_pool_layers = n_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.down_sample_layers = nn.ModuleList([ConvLayer(self.c_in, self.c, self.drop_prob)])\n",
    "        channels = self.c\n",
    "        for _ in range(self.n_pool_layers - 1):\n",
    "            self.down_sample_layers += [ConvLayer(channels, channels * 2, self.drop_prob)]\n",
    "            channels *= 2\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(self.drop_prob)\n",
    "        )\n",
    "\n",
    "        self.up_sample_layers = nn.ModuleList()\n",
    "        for _ in range(self.n_pool_layers - 1):\n",
    "            self.up_sample_layers += [ConvLayer(channels * 2, channels // 2, self.drop_prob)]\n",
    "            channels //= 2\n",
    "        self.up_sample_layers += [ConvLayer(channels * 2, channels, self.drop_prob)]\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // 2, kernel_size=1, ),\n",
    "            nn.Conv2d(channels // 2, self.c_out, kernel_size=1),\n",
    "            nn.Conv2d(self.c_out, self.c_out, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        stack = []\n",
    "        y = x\n",
    "        for layer in self.down_sample_layers:\n",
    "            y = layer(y)\n",
    "            stack.append(y)\n",
    "            y = F.max_pool2d(y, kernel_size=2)\n",
    "\n",
    "        y = self.conv(y)\n",
    "        for layer in self.up_sample_layers:\n",
    "            y = F.interpolate(y, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            y = torch.cat([y, stack.pop()], dim=1)\n",
    "            y = layer(y)\n",
    "\n",
    "        return self.conv2(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, drop_probability: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.drop_probability = drop_probability\n",
    "\n",
    "        self.torch_modules = nn.Sequential(\n",
    "            nn.Conv2d(c_in, c_out, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(c_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(self.drop_probability),\n",
    "            nn.Conv2d(c_out, c_out, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(c_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(self.drop_probability)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.torch_modules(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;33mData loading...\u001b[0m\n",
      "\u001b[0;32mData loaded\u001b[0m\n",
      "\u001b[0;33mConstructing model\u001b[0m\n",
      "\u001b[0;32mConstructed model\u001b[0m\n",
      "\u001b[0;33mStarting training\u001b[0m\n",
      "\u001b[0;32mEPOCH: 0\u001b[0m\n",
      "\u001b[0;31m----------\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 88.00 MiB (GPU 0; 3.95 GiB total capacity; 1.27 GiB already allocated; 56.00 MiB free; 2.53 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f6e743cdd57f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-f6e743cdd57f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0msuccess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"EPOCH: {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madvance_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# scheduler.step(epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f6e743cdd57f>\u001b[0m in \u001b[0;36madvance_epoch\u001b[0;34m(model, data_loader, optimizer)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# print(img_in.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# print(ground_truth.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# print(output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytorch_ssim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSIM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralComp-Group23-LMfX5njL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git-repos/NeuralComp-Group23/UNET.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_sample_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralComp-Group23-LMfX5njL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git-repos/NeuralComp-Group23/layers/conv_layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralComp-Group23-LMfX5njL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralComp-Group23-LMfX5njL/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralComp-Group23-LMfX5njL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralComp-Group23-LMfX5njL/lib/python3.7/site-packages/torch/nn/modules/instancenorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     47\u001b[0m         return F.instance_norm(\n\u001b[1;32m     48\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             self.training or not self.track_running_stats, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/NeuralComp-Group23-LMfX5njL/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minstance_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps)\u001b[0m\n\u001b[1;32m   1683\u001b[0m     return torch.instance_norm(\n\u001b[1;32m   1684\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m         \u001b[0muse_input_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m     )\n\u001b[1;32m   1687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 0; 3.95 GiB total capacity; 1.27 GiB already allocated; 56.00 MiB free; 2.53 MiB cached)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_ssim\n",
    "from UNET import UNet\n",
    "from utils.data_loader import collate_batches, MRIDataset, load_data_path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RED = '\\033[0;31m'\n",
    "GREEN = '\\033[0;32m'\n",
    "YELLOW = '\\033[0;33m'\n",
    "ENDC = '\\033[0m'\n",
    "info = lambda x: print(x)\n",
    "warn = lambda x: print(YELLOW + x + ENDC)\n",
    "success = lambda x: print(GREEN + x + ENDC)\n",
    "error = lambda x: print(RED + x + ENDC)\n",
    "\n",
    "\n",
    "def advance_epoch(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    avg_loss = 0.\n",
    "\n",
    "    for iter, data in enumerate(data_loader):\n",
    "        img_gt, img_und, rawdata_und, masks, norm = data\n",
    "\n",
    "        img_in = Variable(torch.FloatTensor(img_und)).cuda()\n",
    "\n",
    "        ground_truth = Variable(torch.FloatTensor(img_gt)).cuda()\n",
    "        # print(img_in.shape)\n",
    "        # print(ground_truth.shape)\n",
    "        output = model(img_in)\n",
    "        # print(output.shape)\n",
    "        criterion = pytorch_ssim.SSIM()\n",
    "\n",
    "        loss = - criterion(output, ground_truth)\n",
    "        # loss = np.sum(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(-loss.item())\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "\n",
    "    return np.average(losses)\n",
    "\n",
    "\n",
    "def evaluate(device, model, data_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for iter, data in enumerate(data_loader):\n",
    "            img_gt, img_und, rawdata_und, masks, norm = data\n",
    "            img_und = img_und.to(device)\n",
    "            img_gt = img_gt.to(device)\n",
    "            output = model(img_und)\n",
    "\n",
    "            import fastMRI.functions.transforms as T\n",
    "\n",
    "            # target = T.normalize() target\n",
    "            # output = output\n",
    "            # print(norm.shape)\n",
    "\n",
    "            loss = - pytorch_ssim.ssim(output, img_gt)\n",
    "            losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "CENTRE_FRACTION = 0.08\n",
    "ACCELERATION = 4\n",
    "EPSILON = 0.0001\n",
    "GAMMA = 0.1\n",
    "STEP_SIZE = 10\n",
    "BATCH_SIZE = 14\n",
    "NUMBER_EPOCHS = 30\n",
    "NUMBER_POOL_LAYERS = 4\n",
    "DROP_PROB = 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    warn(\"Data loading...\")\n",
    "    try:\n",
    "        data_path_train = '/data/local/NC2019MRI/train'\n",
    "        data_list = load_data_path(data_path_train, data_path_train)\n",
    "    except:\n",
    "        data_path_train = '/home/sam/datasets/FastMRI/NC2019MRI/train'\n",
    "        data_list = load_data_path(data_path_train, data_path_train)\n",
    "\n",
    "    \n",
    "    # Randomly splitting indices:\n",
    "\n",
    "\n",
    "    data_list_train = data_list['train']\n",
    "    data_list_val = data_list['val']\n",
    "\n",
    "    seed = False  # random masks for each slice\n",
    "\n",
    "    num_workers = 8\n",
    "    # create data loader for training set. It applies same to validation set as well\n",
    "    train_dataset = MRIDataset(data_list_train, acceleration=ACCELERATION, center_fraction=CENTRE_FRACTION,\n",
    "                               use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=num_workers,\n",
    "                              collate_fn=collate_batches)\n",
    "\n",
    "    val_dataset = MRIDataset(data_list_val, acceleration=ACCELERATION,\n",
    "                             center_fraction=CENTRE_FRACTION, use_seed=seed)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=num_workers,\n",
    "                            collate_fn=collate_batches)\n",
    "\n",
    "    success(\"Data loaded\")\n",
    "\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    warn(\"Constructing model\")\n",
    "    model = UNet(1, 1, 32, NUMBER_POOL_LAYERS, DROP_PROB).to(device)\n",
    "    success(\"Constructed model\")\n",
    "\n",
    "    criterion = pytorch_ssim.SSIM()\n",
    "\n",
    "    optimiser = optim.Adam(model.parameters(), lr=EPSILON)\n",
    "\n",
    "    warn(\"Starting training\")\n",
    "\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimiser, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    for epoch in range(0, NUMBER_EPOCHS):\n",
    "        success(f\"EPOCH: {epoch}\")\n",
    "        error(\"-\" * 10)\n",
    "        train_loss = advance_epoch(model, train_loader, optimiser)\n",
    "        train_losses.append(train_loss)\n",
    "        # scheduler.step(epoch)\n",
    "        dev_loss = evaluate(device, model, val_loader)\n",
    "        val_losses.append(dev_loss)\n",
    "        info(\n",
    "            f'Epoch = [{epoch:4d}/{NUMBER_EPOCHS:4d}] TrainLoss = {train_loss:.4g} '\n",
    "            f'ValLoss = {dev_loss:.4g}',\n",
    "        )\n",
    "    torch.save(model.state_dict(),\n",
    "               f\"./models/UNET-B{BATCH_SIZE}e-{NUMBER_EPOCHS}-ssim-adam\")\n",
    "    plt.plot(range(NUMBER_EPOCHS), train_losses)\n",
    "    plt.plot(range(NUMBER_EPOCHS), val_losses)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data loading...\")\n",
    "# data_path_train = '/data/local/NC2019MRI/train'\n",
    "data_path_train = '/home/sam/datasets/FastMRI/NC2019MRI/train'\n",
    "data_list = load_data_path(data_path_train, data_path_train)\n",
    "\n",
    "# Split dataset into train-validate\n",
    "validation_split = 0.1\n",
    "dataset_len = len(data_list['train'])\n",
    "indices = list(range(dataset_len))\n",
    "\n",
    "# Randomly splitting indices:\n",
    "val_len = int(np.floor(validation_split * dataset_len))\n",
    "validation_idx = np.random.choice(indices, size=val_len, replace=False)\n",
    "train_idx = list(set(indices) - set(validation_idx))\n",
    "\n",
    "data_list_train = [data_list['train'][i] for i in train_idx]\n",
    "data_list_val = [data_list['val'][i] for i in validation_idx]\n",
    "\n",
    "criterion = pytorch_ssim.SSIM()\n",
    "\n",
    "acc = 8\n",
    "cen_fract = 0.04\n",
    "seed = False  # random masks for each slice\n",
    "num_workers = 8\n",
    "val_loss = []\n",
    "# create data loader for training set. It applies same to validation set as well\n",
    "train_dataset = MRIDataset(data_list_train, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=10, num_workers=num_workers)\n",
    "\n",
    "val_dataset = MRIDataset(data_list_val, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=10, num_workers=num_workers)\n",
    "\n",
    "data_loaders = {\"train\": train_loader, \"val\": val_loader}\n",
    "data_lengths = {\"train\": len(train_idx), \"val\": val_len}\n",
    "train_loss = []\n",
    "print(\"loaded data\")\n",
    "for phase in ['train', 'val']:\n",
    "\n",
    "    for i, sample in enumerate(data_loaders[phase]):\n",
    "        img_gt, img_und, rawdata_und, masks, norm = sample\n",
    "        img_in = T.center_crop(T.complex_abs(img_und).unsqueeze(0), [320, 320]).transpose_(0, 1)\n",
    "\n",
    "        loss = - criterion(img_in,\n",
    "                           T.center_crop(T.complex_abs(img_gt).unsqueeze(0), [320, 320]).transpose_(0, 1))\n",
    "\n",
    "        # backward + optimise only if in training phase\n",
    "\n",
    "        # calculate loss\n",
    "        # tra.append(- loss.item())\n",
    "        # running_loss += - loss.item() * img_in.size(0)\n",
    "\n",
    "    # epoch_loss = running_loss / data_lengths[phase]\n",
    "    # print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "\n",
    "    if phase == 'train':\n",
    "        train_loss.append(-loss.item())\n",
    "    else:\n",
    "        val_loss.append(-loss.item())\n",
    "\n",
    "print(train_loss)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_train = '/home/sam/datasets/FastMRI/NC2019MRI/train'\n",
    "# data_path_train = '/data/local/NC2019MRI/train'\n",
    "\n",
    "data_path_val = data_path_train\n",
    "data_list = load_data_path(data_path_train, data_path_val)\n",
    "\n",
    "acc = 4\n",
    "cen_fract = 0.08\n",
    "seed = False  # random masks for each slice\n",
    "num_workers = 8\n",
    "\n",
    "val_dataset = MRIDataset(data_list['val'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=1, num_workers=num_workers, collate_fn=collate_batches)\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "optimiser = 'SGD'\n",
    "model = UNET.UNet(1,1,32,4,0).to(device)\n",
    "model.load_state_dict(torch.load(f\"./vary-optim/models/UNET-lr0.0001-{optimiser}.pkl\"))\n",
    "model.eval()\n",
    "fig = plt.figure()\n",
    "counter= 0\n",
    "ssims = []\n",
    "for i, sample in enumerate(val_loader):\n",
    "    img_gt, img_und, rawdata_und, masks, norm = sample\n",
    "    img_in = img_und.to(device)\n",
    "    # img_in = T.center_crop(T.complex_abs(img_und).unsqueeze(0), [320, 320]).transpose(0,1).to(device)\n",
    "\n",
    "    # input\n",
    "    A = img_und.squeeze()\n",
    "    # print(A.shape)\n",
    "    \n",
    "    # output\n",
    "    output = model(img_in)\n",
    "    # print(output.shape)\n",
    "    B = output.squeeze().cpu()\n",
    "    # print(B.shape)\n",
    "    ssim = pytorch_ssim.ssim(output,img_gt.to(device))\n",
    "    \n",
    "    ssims.append(ssim.item())\n",
    "    # print(f\"SSIM of this image is {ssim}\")\n",
    "    # real\n",
    "    C = img_gt.squeeze()\n",
    "    # print(C.shape)\n",
    "    all_imgs = torch.stack([A.detach(), B.detach(), C.detach()], dim=0)\n",
    "\n",
    "    # from left to right: mask, masked kspace, undersampled image, ground truth\n",
    "    if ssim > 0.9 and counter <=3:\n",
    "        show_slices(all_imgs, [0, 1, 2], cmap='gray')\n",
    "        # plt.savefig(f\"./vary-optim/reconstructions/{optimiser}-ssim/{ssim:.2f}-{optimiser}30.png\")\n",
    "        plt.pause(1)\n",
    "        counter += 1\n",
    "\n",
    "    # if counter >= 3: break  # show 4 random slices\n",
    "print(f\"Max ssim : {max(ssims)}\")\n",
    "print(f\"Average ssim is : {np.average(ssims)}\")\n",
    "print(f\"Variance in ssim is {np.var(ssims)}\")\n",
    "print(f\"Standard Deviation in ssim is {np.std(ssims)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
